# Guide complet DSLR

Documentation d√©taill√©e de chaque fonction et r√©sultat du projet.

---

## üìä 1. Analyse descriptive - `describe.py`

### Fonction
Calcule les statistiques descriptives pour chaque feature num√©rique du dataset.

### Statistiques calcul√©es (11 au total)

1. **Count** : Nombre de valeurs non-nulles
   - Permet d'identifier les donn√©es manquantes

2. **Mean** (moyenne arithm√©tique) : $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$
   - Somme de toutes les valeurs divis√©e par le nombre de valeurs
   - Centre de gravit√© des donn√©es
   - Sensible aux valeurs extr√™mes (outliers)

3. **Std** (√©cart-type) : $\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}$
   - Mesure la dispersion des valeurs autour de la moyenne
   - Plus l'√©cart-type est √©lev√©, plus les donn√©es sont dispers√©es
   - M√™me unit√© que les donn√©es d'origine
   - ~68% des valeurs sont dans [mean - std, mean + std] (loi normale)

4. **Min** : Valeur minimum du dataset

5. **25%** (Q1 - Premier quartile)
   - 25% des valeurs sont inf√©rieures ou √©gales √† Q1
   - Borne inf√©rieure de la bo√Æte dans un boxplot

6. **50%** (Q2 - M√©diane)
   - Valeur centrale qui divise les donn√©es en deux parties √©gales
   - 50% des valeurs sont en dessous, 50% au-dessus
   - Moins sensible aux outliers que la moyenne
   - Si m√©diane ‚âà moyenne ‚Üí distribution sym√©trique

7. **75%** (Q3 - Troisi√®me quartile)
   - 75% des valeurs sont inf√©rieures ou √©gales √† Q3
   - Borne sup√©rieure de la bo√Æte dans un boxplot
   - **IQR** (Interquartile Range) = Q3 - Q1 (50% central des donn√©es)

8. **Max** : Valeur maximum du dataset

#### Statistiques avanc√©es
9. **Range** : Max - Min
   - √âtendue totale du dataset
   - Tr√®s sensible aux outliers

10. **Skewness** (asym√©trie) : Mesure de la dissym√©trie de la distribution
    - < 0 : asym√©trique gauche (queue √† gauche, masse √† droite)
    - ‚âà 0 : sym√©trique (distribution normale)
    - \> 0 : asym√©trique droite (queue √† droite, masse √† gauche)

11. **Kurtosis** (aplatissement) : Mesure l'√©paisseur des queues
    - < 0 : queues l√©g√®res (platykurtique, moins d'outliers)
    - ‚âà 0 : distribution normale (m√©sokurtique)
    - \> 0 : queues lourdes (leptokurtique, plus d'outliers)

### Utilit√© des statistiques
#### Statistiques de base (Mean, Std, Min, Max, Quartiles)
- **Comprendre la distribution** : Mean et Median donnent le centre des donn√©es
- **Mesurer la dispersion** : Std et IQR indiquent la variabilit√©
- **D√©tecter les valeurs extr√™mes** : Comparer Min/Max avec les quartiles
- **Identifier les donn√©es manquantes** : Count < nombre total de lignes

#### Statistiques avanc√©es (Range, Skewness, Kurtosis)

**Range (√âtendue)** : Max - Min
- **D√©tecter les outliers** : Range >> Std ‚Üí pr√©sence probable de valeurs extr√™mes
- **√âvaluer la robustesse** : Grande range = donn√©es sensibles aux valeurs aberrantes
- **Choisir la normalisation** : Range importante ‚Üí privil√©gier Z-score (mean/std)
- Exemple : Arithmancy a Range ‚âà 20,000 ‚Üí valeurs sur une tr√®s large √©chelle

**Skewness (Asym√©trie)**
- **Identifier les biais** : Skewness ‚â† 0 ‚Üí distribution d√©s√©quilibr√©e
  - Skew > 0 : Beaucoup de petites valeurs, quelques grandes (ex: salaires)
  - Skew < 0 : Beaucoup de grandes valeurs, quelques petites (ex: √¢ge de d√©c√®s)
- **Choisir les transformations** : Skewness √©lev√©e ‚Üí appliquer log() ou sqrt()
- **Interpr√©ter mean vs median** : Si Skew > 0, alors Mean > Median (et inversement)
- **Pr√©parer les mod√®les** : Beaucoup d'algorithmes ML supposent une sym√©trie

**Kurtosis (√âpaisseur des queues)**
- **D√©tecter les outliers fr√©quents** : Kurtosis > 0 ‚Üí beaucoup de valeurs extr√™mes
- **√âvaluer les risques** : Queues lourdes ‚Üí √©v√©nements extr√™mes plus probables
- **V√©rifier la normalit√©** : Kurtosis ‚âà 0 ‚Üí distribution proche de la normale
- **Choisir les tests statistiques** : Kurtosis √©lev√©e ‚Üí √©viter les tests param√©triques
- Exemple : En finance, Kurtosis > 0 indique des crashs/pics fr√©quents

#### Utilisation combin√©e
- **Normalit√©** : Mean ‚âà Median + Skew ‚âà 0 + Kurtosis ‚âà 0 ‚Üí distribution normale
- **Qualit√© des donn√©es** : Range/Std tr√®s √©lev√© + Kurtosis > 0 ‚Üí nettoyer les outliers
- **S√©lection de features** : Skewness et Kurtosis extr√™mes ‚Üí feature peu fiable ou √† transformer

---

## üìà 2. Visualisation - Histogrammes
### Script : `histogram.py`
### Fonction
Affiche les histogrammes de distribution pour tous les cours, avec une couleur par maison.

### Question pos√©e
**Quel cours a la r√©partition de notes la plus homog√®ne entre les 4 maisons ?**

**Care of Magical Creatures**
Les 4 maisons ont des distributions tr√®s similaires pour ce cours (m√™me moyenne, m√™me dispersion).

### Utilit√©
- Identifier les features non-discriminantes
- Visualiser les distributions par classe
- Comprendre la s√©parabilit√© des donn√©es

---

## üîó 3. Visualisation - Scatter plots

### Script : `scatter_plot.py`

### Fonction
Affiche les **12 paires de features les plus corr√©l√©es** sous forme de scatter plots pour identifier les redondances.

### Qu'est-ce que la corr√©lation ?

La **corr√©lation** mesure la force et la direction de la relation lin√©aire entre deux variables.

**Coefficient de corr√©lation de Pearson** : $r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}$

### Interpr√©tation des valeurs (de -1 √† +1)

| Valeur de r | Signification | Relation visuelle sur scatter plot |
|-------------|---------------|-------------------------------------|
| **r = +1** | Corr√©lation positive parfaite | Points parfaitement align√©s sur une droite montante (‚Üó) |
| **r > 0.7** | Forte corr√©lation positive | Points proches d'une droite montante |
| **r ‚âà 0.5** | Corr√©lation positive mod√©r√©e | Tendance montante visible mais dispers√©e |
| **r ‚âà 0** | Pas de corr√©lation lin√©aire | Nuage de points dispers√© sans tendance |
| **r ‚âà -0.5** | Corr√©lation n√©gative mod√©r√©e | Tendance descendante visible mais dispers√©e |
| **r < -0.7** | Forte corr√©lation n√©gative | Points proches d'une droite descendante |
| **r = -1** | Corr√©lation n√©gative parfaite | Points parfaitement align√©s sur une droite descendante (‚Üò) |

### Pourquoi de -1 √† +1 ?

Le coefficient est **normalis√©** :
- **Covariance** (num√©rateur) : mesure brute de la relation (peut √™tre √©norme)
- **√âcarts-types** (d√©nominateur) : normalisent pour obtenir une √©chelle fixe
- R√©sultat : toujours entre -1 et +1, ind√©pendamment des unit√©s

**Propri√©t√©s importantes** :
- r ne mesure que les relations **lin√©aires** (peut rater les courbes, paraboles, etc.)
- r est **sym√©trique** : cor(X, Y) = cor(Y, X)
- r est **sans unit√©** : m√™me r√©sultat en m√®tres, km, ou miles

### Visualisation du script

Le script affiche un graphique avec **12 scatter plots** :
- Class√©s par corr√©lation d√©croissante (du plus corr√©l√© au moins corr√©l√©)
- La paire avec r le plus √©lev√© est mise en √©vidence (bordure rouge, titre en gras)
- Permet de voir visuellement la relation lin√©aire entre chaque paire
- Focus sur l'objectif : identifier **LES 2 features les plus similaires**

**Approche cibl√©e** : Plut√¥t que d'afficher toutes les combinaisons possibles (13√ó12/2 = 78 paires), le script se concentre sur les 12 paires les plus prometteuses pour faciliter la lecture.

### Question pos√©e (selon le sujet du projet)
**What are the two features that are similar?**

### R√©sultat
**Astronomy & Defense Against the Dark Arts** (corr√©lation r = 1.0)

Ces deux features sont **parfaitement corr√©l√©es lin√©airement** :
- Quand Astronomy augmente d'1 point, Defense augmente proportionnellement
- Elles contiennent exactement la m√™me information
- **Conclusion** : on peut en supprimer une sans perte d'information (√©vite la redondance)

### Utilit√©
- ‚úÖ **D√©tecter la multicolin√©arit√©** : features r > 0.9 ‚Üí redondance
- ‚úÖ **R√©duire la dimensionnalit√©** : supprimer les doublons
- ‚úÖ **Comprendre les relations** : quels cours sont li√©s
- ‚úÖ **√âviter l'overfitting** : moins de features corr√©l√©es ‚Üí meilleure g√©n√©ralisation

---

## üé® 4. Visualisation - Pair plot

### Script : `pair_plot.py`

### Qu'est-ce qu'un pair plot ?

Le sujet demande : **"displays a pair plot OR scatter plot matrix"** (nous avons choisi le **pair plot** car plus visuel et intuitif)

Un **pair plot** est une grille de graphiques qui affiche :
- **Diagonale** : Histogrammes de distribution pour chaque feature
- **Hors diagonale** : Scatter plots entre toutes les paires de features, **color√©s par maison**

C'est un outil puissant pour visualiser simultan√©ment :
- Les distributions individuelles de chaque variable (histogrammes)
- Les relations entre paires de variables (scatter plots)
- Les **clusters/s√©parations entre classes** (ici : les 4 maisons avec couleurs distinctes)

**Avantage du pair plot** : Plus intuitif qu'une matrice de corr√©lation num√©rique, car on voit directement les s√©parations visuelles entre les maisons.

### Algorithme de s√©lection des features

**√âtape 1 : Calcul du score de s√©parabilit√©**

Pour chaque feature, on calcule :

$$\text{Score} = \frac{\text{Variance inter-maisons}}{\text{Moyenne des variances intra-maisons}} = \frac{\sigma^2(\bar{x}_{\text{houses}})}{\text{mean}(\sigma^2_{\text{Gryffindor}}, \sigma^2_{\text{Slytherin}}, ...)}$$

- **Num√©rateur** : Variance des moyennes de chaque maison
  - Mesure √† quel point les maisons ont des moyennes diff√©rentes
  - Grande variance ‚Üí les maisons sont bien s√©par√©es
  
- **D√©nominateur** : Moyenne des variances √† l'int√©rieur de chaque maison
  - Mesure la dispersion des √©l√®ves au sein de leur maison
  - Petite variance ‚Üí les √©l√®ves d'une maison sont homog√®nes

**Interpr√©tation** :
- **Score √©lev√©** ‚Üí Feature discriminante (s√©pare bien les maisons)
- **Score faible** ‚Üí Feature peu utile (les maisons se chevauchent)

**√âtape 2 : S√©lection**
- Trier les features par score d√©croissant
- S√©lectionner les **5 meilleures**
- G√©n√©rer le pair plot pour ces features uniquement

**√âtape 3 : Visualisation**
- Matrice 5√ó5 = 25 graphiques
- Couleur par maison (rouge, vert, bleu, jaune)
- Permet de voir quelles paires de features s√©parent le mieux les maisons

### R√©sultat

Le script g√©n√®re un **pair plot** (choix "pair plot" du sujet : *"pair plot OR scatter plot matrix"*) pour r√©pondre √† la question : **"which features are you going to use for your logistic regression?"**

**Pair plot des top 6 features s√©lectionn√©es** :

**Top 6 features s√©lectionn√©es** :
1. **Astronomy** : Score le plus √©lev√©
2. **Herbology** : Tr√®s discriminante
3. **Defense Against the Dark Arts** : Corr√©l√©e avec Astronomy mais utile
4. **Ancient Runes** : Bonne s√©paration
5. **Charms** : Compl√®te le top 5
6. **(Une 6√®me feature selon le score)**

**Observations visuelles sur le pair plot** :
- **Diagonale** : Histogrammes montrant la distribution de chaque feature par maison
- **Hors diagonale** : Scatter plots avec **clusters color√©s** (rouge = Gryffindor, vert = Slytherin, bleu = Ravenclaw, jaune = Hufflepuff)
- Certaines paires (ex: Astronomy vs Herbology) montrent des **groupes bien s√©par√©s**
- On voit imm√©diatement quelles features discriminent le mieux les maisons

### R√©ponse √† la question du sujet
**"Which features are you going to use for your logistic regression?"**

‚Üí Les **5 meilleures features** : Astronomy, Herbology, Defense Against the Dark Arts, Ancient Runes, Charms

**Justification visible dans le pair plot** :
- ‚úÖ **Score √©lev√©** ‚Üí variance inter-maisons >> variance intra-maisons
- ‚úÖ **Clusters visuellement s√©par√©s** ‚Üí on voit 4 groupes de couleurs distincts
- ‚úÖ **Distributions diff√©rentes** ‚Üí histogrammes d√©cal√©s entre maisons
- ‚úÖ **Pas de redondance** ‚Üí d√©fense contre l'overfitting (on garde 5/13 features)

### Utilit√©

**Avant l'entra√Ænement** :
- ‚úÖ S√©lectionner les features les plus pertinentes (√©vite le sur-apprentissage)
- ‚úÖ R√©duire la dimensionnalit√© (5/13 features suffisent)
- ‚úÖ √âliminer les features redondantes (corr√©l√©es)

**Analyse exploratoire** :
- ‚úÖ Visualiser les clusters par maison
- ‚úÖ Identifier les relations non-lin√©aires entre features
- ‚úÖ D√©tecter les outliers (points isol√©s)

**Pour le mod√®le** :
- ‚úÖ Features s√©lectionn√©es ‚Üí meilleure g√©n√©ralisation
- ‚úÖ Moins de features ‚Üí entra√Ænement plus rapide
- ‚úÖ Score √©lev√© ‚Üí garantie de s√©parabilit√© lin√©aire

---

## üéì 5. Entra√Ænement - `logreg_train.py`

### Fonction
Entra√Æne un mod√®le de r√©gression logistique multi-classe avec la strat√©gie **One-vs-All**.

### Algorithme : Batch Gradient Descent

```
Pour chaque maison H :
  1. Cr√©er un probl√®me binaire (H vs tous les autres)
  2. Initialiser les poids √† 0
  3. Pour 1000 √©poques :
     a. Calculer les pr√©dictions : œÉ(w¬∑x)
     b. Calculer le gradient sur TOUT le dataset
     c. Mettre √† jour les poids : w = w - Œ±¬∑‚àáL
```

### Param√®tres
- **Learning rate** : 0.5
- **√âpoques** : 1000
- **Mises √† jour** : 1 par √©poque = **1000 total**
- **Features** : 5 (Astronomy, Herbology, Defense, Ancient Runes, Charms)

### R√©sultat
- **Fichier** : `weights.json`
- **Pr√©cision** : >98% sur le test set
- **Contenu** : 4 mod√®les binaires (Gryffindor, Hufflepuff, Ravenclaw, Slytherin)

### Structure du fichier `weights.json`
```json
{
  "features": ["Astronomy", "Herbology", ...],
  "normalization": {
    "mean": [...],
    "std": [...]
  },
  "houses": {
    "Gryffindor": {"weights": [...], "bias": 0.123},
    "Hufflepuff": {"weights": [...], "bias": -0.456},
    ...
  }
}
```

---

## üîÆ 6. Pr√©diction - `logreg_predict.py`

### Fonction
Pr√©dit la maison de chaque √©l√®ve et g√©n√®re des visualisations automatiques.

### Algorithme

```
Pour chaque √©l√®ve :
  1. Normaliser ses features (avec mean/std du training)
  2. Pour chaque maison H :
     a. Calculer le score : œÉ(w_H ¬∑ x + b_H)
  3. Choisir la maison avec le score maximal
  4. √âcrire dans houses.csv
```

### R√©sultat
1. **Fichier** : `houses.csv` (2 colonnes : Index, Hogwarts House)
2. **Graphiques automatiques** :
   - Bar chart : Nombre de pr√©dictions par maison
   - Pie chart : R√©partition en pourcentage
3. **Statistiques** :
   ```
   Gryffindor: 94 (23.5%)
   Hufflepuff: 100 (25.0%)
   Ravenclaw: 92 (23.0%)
   Slytherin: 76 (19.0%)
   Total: 362/400 pr√©dictions
   ```

### Gestion des valeurs manquantes
Les √©l√®ves avec features manquantes sont **ignor√©s** ‚Üí 362 pr√©dictions sur 400 test samples.

---

## üìê Formules math√©matiques cl√©s

### R√©gression logistique (fonction sigmo√Øde)
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

### Score de pr√©diction
$$z = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n = w^T x + b$$

### Log-Loss (fonction de co√ªt)
$$L = -\frac{1}{m} \sum_{i=1}^{m} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]$$

### Gradient
$$\frac{\partial L}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i) x_{ij}$$

### Mise √† jour des poids
$$w_j := w_j - \alpha \frac{\partial L}{\partial w_j}$$

---

## üìö Concepts machine learning expliqu√©s

### One-vs-All (OvA)
Strat√©gie pour la classification multi-classe :
- 4 maisons ‚Üí 4 mod√®les binaires
- Mod√®le 1 : Gryffindor vs (Hufflepuff + Ravenclaw + Slytherin)
- Mod√®le 2 : Hufflepuff vs (autres)
- Mod√®le 3 : Ravenclaw vs (autres)
- Mod√®le 4 : Slytherin vs (autres)
- Pr√©diction finale : maison avec le score maximal

### Normalisation (Z-score)
```
x_normalized = (x - mean) / std
```
Pourquoi ? √âviter que les features avec grandes valeurs dominent le gradient.

### Learning rate (Œ±)
Contr√¥le la taille des pas lors de la descente de gradient :
- Trop petit ‚Üí convergence tr√®s lente
- Trop grand ‚Üí divergence (oscillations)
---

## üîç Analyse des performances

### Pr√©cision >98%
Sur 400 test samples, le mod√®le pr√©dit correctement >392 maisons.

### Gestion des donn√©es manquantes
38 √©l√®ves ignor√©s (362/400 pr√©dictions) car features manquantes.

### Features s√©lectionn√©es (5/13)
Seules les 5 features les plus discriminantes sont utilis√©es :
1. Astronomy
2. Herbology
3. Defense Against the Dark Arts
4. Ancient Runes
5. Charms

‚Üí R√©duit le sur-apprentissage et am√©liore la g√©n√©ralisation.

